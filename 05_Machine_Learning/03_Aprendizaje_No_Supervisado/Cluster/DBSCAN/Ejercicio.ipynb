{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio Clustering\n",
    "El objetivo es implementar un modelo que agrupa las transacciones apropiadamente y encontrar los potenciales outliers, es decir, aquellas transacciones que son sospechosas de ser un fraude o un error. Para resolver este ejercicio correctamente hay que investigar, en vez de simplemente seguir a rajatabla lo enseñado en el curso.\n",
    "\n",
    "**Pistas:**\n",
    "\n",
    "- Hemos explicado un algoritmo de clustering que no solo asigna elementos a clusters válidos, sino que también clasifica elementos como valores extremos (outliers). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"./Datos/CC General.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = df.CUST_ID\n",
    "df = df.drop(columns=\"CUST_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MINIMUM_PAYMENTS'].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X_train=df[~df[\"MINIMUM_PAYMENTS\"].isna()].drop(columns=[\"CREDIT_LIMIT\",\"MINIMUM_PAYMENTS\"])\n",
    "y_train=df[~df[\"MINIMUM_PAYMENTS\"].isna()][\"MINIMUM_PAYMENTS\"]\n",
    "X_test=df[df[\"MINIMUM_PAYMENTS\"].isna()].drop(columns=[\"CREDIT_LIMIT\",\"MINIMUM_PAYMENTS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol=DecisionTreeRegressor()\n",
    "arbol.fit(X_train,y_train)\n",
    "y_predicho=arbol.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat_importances = pd.DataFrame(arbol.feature_importances_, index=X_train.columns, columns=[\"Importance\"])\n",
    "feat_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feat_importances.plot(kind='bar', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CREDIT_LIMIT'].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputamos a 0 los valores nulos, así podemos ver si los elementos anómalos son aquellos que tienen estas columnas a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizo el algoritmo DBSCAN.\n",
    "Y utilizo el StandardScaler para que todos los valores esten en la misma escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalizado = pd.DataFrame(StandardScaler().fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = DBSCAN()\n",
    "cluster_labels = clusterer.fit_predict(df_normalizado)\n",
    "## muestro el top 5\n",
    "pd.Series(cluster_labels).value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cluster_labels).value_counts(normalize=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN etiqueta practicamente un 75% de valores como -1. Usamos el coeficiente de silueta (`silhouette_score`) para ver como etiqueta el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(df_normalizado, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor es negativo, con lo que quiere decir que la segmetnación es bastante mala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Veamos los hiperparamentros que hemos utilizado\n",
    "DBSCAN().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar un similar a GridSearch para este modelo. ya que al no tener predict no podemos usar este o RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform,randint\n",
    "\n",
    "distribucion_parametros = {\n",
    "    \"eps\": uniform(0,5),\n",
    "    \"min_samples\": randint(2, 20),\n",
    "    \"p\": randint(1, 3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "n_muestras = 30 # probamos 30 combinaciones de hiperparámetros\n",
    "n_iteraciones = 2 #para validar, vamos a entrenar para cada \n",
    "                  # selección de hiperparámetros en 2 muestras distintas\n",
    "pct_muestra = 0.7 # usamos el 70% de los datos para entrenar el modelo en cada iteracion\n",
    "resultados_busqueda = []\n",
    "lista_parametros = list(ParameterSampler(distribucion_parametros, n_iter=n_muestras))\n",
    "\n",
    "for param in lista_parametros:\n",
    "    for iteration in range(n_iteraciones):\n",
    "        param_resultados = []\n",
    "        muestra = df_normalizado.sample(frac=pct_muestra)\n",
    "        etiquetas_clusters = DBSCAN(n_jobs=-1, **param).fit_predict(muestra)\n",
    "        try:\n",
    "            param_resultados.append(silhouette_score(muestra, etiquetas_clusters))\n",
    "        except ValueError: # a veces silhouette_score falla en los casos en los que solo hay 1 cluster\n",
    "            pass\n",
    "    puntuacion_media = np.mean(param_resultados)\n",
    "    resultados_busqueda.append([puntuacion_media, param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parametros=sorted(resultados_busqueda, key=lambda x: x[0], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cojo el primer valor que he obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_params = Parametros[0][1]\n",
    "\n",
    "clusterer = DBSCAN(n_jobs=-1, **mejores_params)\n",
    "\n",
    "etiquetas_cluster = clusterer.fit_predict(df_normalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(etiquetas_cluster).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay 63 que se consideran anomalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumen_cluster(cluster_id):\n",
    "    cluster = df[etiquetas_cluster==cluster_id]\n",
    "    resumen_cluster = cluster.mean().to_dict()\n",
    "    resumen_cluster[\"cluster_id\"] = cluster_id\n",
    "    return resumen_cluster\n",
    "\n",
    "def comparar_clusters(*cluster_ids):\n",
    "    resumenes = []\n",
    "    for cluster_id in cluster_ids:\n",
    "        resumenes.append(resumen_cluster(cluster_id))\n",
    "    return pd.DataFrame(resumenes).set_index(\"cluster_id\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparar_clusters(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
